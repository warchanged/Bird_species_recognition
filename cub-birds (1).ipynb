{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5140550,"sourceType":"datasetVersion","datasetId":2534241}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install open_clip_torch torch torchvision torchaudio pandas scikit-learn tqdm ftfy regex transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:52:44.482975Z","iopub.execute_input":"2025-05-07T09:52:44.483291Z","iopub.status.idle":"2025-05-07T09:53:58.625241Z","shell.execute_reply.started":"2025-05-07T09:52:44.483261Z","shell.execute_reply":"2025-05-07T09:53:58.624430Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport open_clip\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms # We might use CLIP's specific transforms\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split # If train_test_split.txt is not used\nfrom sklearn.metrics import accuracy_score\nimport re\nimport torch.optim as optim\n# Import the scheduler\nfrom transformers import get_cosine_schedule_with_warmup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:53:58.626690Z","iopub.execute_input":"2025-05-07T09:53:58.626935Z","iopub.status.idle":"2025-05-07T09:54:12.379474Z","shell.execute_reply.started":"2025-05-07T09:53:58.626912Z","shell.execute_reply":"2025-05-07T09:54:12.378930Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/cub2002011/CUB_200_2011/\"\nIMAGE_DIR = os.path.join(DATA_DIR, \"images\")\n\n# Load image paths and IDs\nimages_df = pd.read_csv(os.path.join(DATA_DIR, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n\n# Load image class labels\nimage_class_labels_df = pd.read_csv(os.path.join(DATA_DIR, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n\n# Load class names\nclasses_df = pd.read_csv(os.path.join(DATA_DIR, 'classes.txt'), sep=' ', names=['class_id', 'classname'])\n# Preprocess class names (e.g., \"001.Black_footed_Albatross\" -> \"black footed albatross\")\nclasses_df['classname'] = classes_df['classname'].apply(lambda x: x.split('.')[-1].replace('_', ' ').lower())\n\n# Load train/test split\ntrain_test_split_df = pd.read_csv(os.path.join(DATA_DIR, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_training_img'])\n\n# Merge dataframes\ndata_df = images_df.merge(image_class_labels_df, on='img_id')\ndata_df = data_df.merge(classes_df, on='class_id')\ndata_df = data_df.merge(train_test_split_df, on='img_id')\n\n# Split into train and test sets\ntrain_df = data_df[data_df['is_training_img'] == 1].reset_index(drop=True)\ntest_df = data_df[data_df['is_training_img'] == 0].reset_index(drop=True)\n\nprint(f\"Total images: {len(data_df)}\")\nprint(f\"Training images: {len(train_df)}\")\nprint(f\"Testing images: {len(test_df)}\")\nprint(f\"Number of classes: {classes_df['class_id'].nunique()}\")\ndata_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:12.380113Z","iopub.execute_input":"2025-05-07T09:54:12.380453Z","iopub.status.idle":"2025-05-07T09:54:12.469614Z","shell.execute_reply.started":"2025-05-07T09:54:12.380436Z","shell.execute_reply":"2025-05-07T09:54:12.468935Z"}},"outputs":[{"name":"stdout","text":"Total images: 11788\nTraining images: 5994\nTesting images: 5794\nNumber of classes: 200\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   img_id                                           filepath  class_id  \\\n0       1  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n1       2  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n2       3  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n3       4  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n4       5  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n\n                classname  is_training_img  \n0  black footed albatross                0  \n1  black footed albatross                1  \n2  black footed albatross                0  \n3  black footed albatross                1  \n4  black footed albatross                1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img_id</th>\n      <th>filepath</th>\n      <th>class_id</th>\n      <th>classname</th>\n      <th>is_training_img</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Simple templates used in many CLIP papers\nprompt_templates = [\n    'a photo of a {}.',\n    'a bad photo of a {}.',\n    'a photo of the {}.',\n    'a picture of a {}.',\n    'a bird called {}.',\n    'the bird {}.',\n    'a type of bird called {}.',\n    '{} bird.',\n]\n\ndef create_prompt(class_name, template):\n    return template.format(class_name)\n\n# Create prompts for all classes for zero-shot evaluation\nall_class_names = classes_df['classname'].tolist()\nnum_classes = len(all_class_names)\nzeroshot_prompts = [create_prompt(c, prompt_templates[0]) for c in all_class_names]\nprint(f\"Example prompt: {zeroshot_prompts[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:12.470405Z","iopub.execute_input":"2025-05-07T09:54:12.470760Z","iopub.status.idle":"2025-05-07T09:54:12.475794Z","shell.execute_reply.started":"2025-05-07T09:54:12.470736Z","shell.execute_reply":"2025-05-07T09:54:12.475021Z"}},"outputs":[{"name":"stdout","text":"Example prompt: a photo of a black footed albatross.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class CUBDataset(Dataset):\n    def __init__(self, df, image_dir, transform, tokenizer, templates, augment_templates=False):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.templates = templates\n        self.augment_templates = augment_templates # Flag for template augmentation\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row['filepath'])\n        class_name = row['classname']\n        class_id = row['class_id'] - 1 # 0-indexed class ID\n\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image_tensor = self.transform(image)\n        except Exception as e:\n            # Optional: Add a print here only for debugging initial runs if needed\n            # print(f\"Error loading image {img_path}: {e}. Using placeholder.\")\n            try: # Attempt to load the first image as a fallback\n                 placeholder_path = os.path.join(self.image_dir, self.df.iloc[0]['filepath'])\n                 image = Image.open(placeholder_path).convert('RGB')\n                 image_tensor = self.transform(image)\n            except: # If even that fails, return None\n                 # Returning None requires a collate_fn that handles it (added later)\n                 return None # Indicate failure\n\n        # Create prompt for this specific image\n        # For FLYP-style augmentation, choose a random template during training\n        if self.augment_templates: # Check the flag directly\n             template = np.random.choice(self.templates)\n             # Optional Debug Print (remove after verification):\n             # if idx < 5: print(f\"Idx {idx}, Template used: '{template}'\")\n        else:\n             template = self.templates[0] # Default template otherwise\n\n        text_prompt = create_prompt(class_name, template)\n        # Note: open_clip tokenizer returns a tensor batch; we need the first element [0]\n        tokenized_text = self.tokenizer(text_prompt)[0]\n\n        return image_tensor, tokenized_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:12.477580Z","iopub.execute_input":"2025-05-07T09:54:12.477779Z","iopub.status.idle":"2025-05-07T09:54:12.494111Z","shell.execute_reply.started":"2025-05-07T09:54:12.477764Z","shell.execute_reply":"2025-05-07T09:54:12.493565Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# --- Model Setup ---\nmodel_name = 'ViT-B-32'\npretrained_dataset = 'laion2b_s34b_b79k'\n\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n    model_name,\n    pretrained=pretrained_dataset,\n    device=device,\n    jit=False\n)\ntokenizer = open_clip.get_tokenizer(model_name)\n\nfor param in model.parameters():\n    param.requires_grad = True\n\n# --- Loss Function ---\nloss_fn = open_clip.ClipLoss(\n    local_loss=False,\n    gather_with_grad=False,\n    cache_labels=True,\n    rank=0,\n    world_size=1\n)\n\n# --- Optimizer ---\n# *** Optimization: Lower LR ***\nlearning_rate = 5e-7\nweight_decay = 0.1\nbetas = (0.9, 0.98)\neps = 1e-6\n\nparams_to_optimize = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = optim.AdamW(\n    params_to_optimize,\n    lr=learning_rate,\n    betas=betas,\n    eps=eps,\n    weight_decay=weight_decay\n)\n\n# --- Scheduler ---\n# *** Optimization: Increased Epochs ***\nnum_epochs = 15\n# Scheduler will be created *after* train_loader in the next cell block\nscheduler = None\nwarmup_steps = 100 # Number of warmup steps (adjust if needed, e.g., len(train_loader)//2 for half an epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:12.494759Z","iopub.execute_input":"2025-05-07T09:54:12.495000Z","iopub.status.idle":"2025-05-07T09:54:17.102827Z","shell.execute_reply.started":"2025-05-07T09:54:12.494974Z","shell.execute_reply":"2025-05-07T09:54:17.102034Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f281b3637d44620a4e6914050254d74"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"batch_size = 64 # Adjust based on GPU memory\nnum_workers = 2 # Adjust based on system capability\n\n# --- Training Dataset ---\n# *** Optimization: Ensure template augmentation is enabled ***\ntrain_dataset = CUBDataset(train_df, IMAGE_DIR, preprocess_train, tokenizer, prompt_templates, augment_templates=True)\n\n# Define a simple collate function to handle potential None items from dataset\ndef safe_collate(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    if not batch:\n        return None # Return None if the whole batch is problematic\n    try:\n        return torch.utils.data.dataloader.default_collate(batch)\n    except Exception as e:\n        # print(f\"Error in collate_fn: {e}\") # Optional: for debugging\n        return None # Return None on collation error\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True, collate_fn=safe_collate)\n\n# --- Test Dataset for Evaluation (Zero-shot style) ---\n# *** THIS CLASS DEFINITION WAS MISSING IN THE PREVIOUS BREAKDOWN ***\nclass CUBTestDataset(Dataset):\n    def __init__(self, df, image_dir, transform):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row['filepath'])\n        class_id = row['class_id'] - 1 # 0-indexed\n\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image_tensor = self.transform(image)\n        except Exception as e:\n            # print(f\"Error loading image {img_path}: {e}. Using placeholder.\") # Optional: for debugging\n            # Fallback logic\n            try:\n                placeholder_path = os.path.join(self.image_dir, self.df.iloc[0]['filepath'])\n                image = Image.open(placeholder_path).convert('RGB')\n                image_tensor = self.transform(image)\n            except:\n                image_tensor = torch.zeros((3, 224, 224)) # Adjust size if needed\n\n        return image_tensor, torch.tensor(class_id, dtype=torch.long)\n\ntest_dataset_eval = CUBTestDataset(test_df, IMAGE_DIR, preprocess_val)\ntest_loader_eval = DataLoader(test_dataset_eval, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=safe_collate)\n\nprint(f\"Train loader size: {len(train_loader)} batches\")\nprint(f\"Test loader size: {len(test_loader_eval)} batches\")\n\n# --- Create Scheduler (Now that train_loader is defined) ---\n# *** Optimization: Create scheduler with warmup and cosine decay ***\nif len(train_loader) > 0:\n    total_steps = len(train_loader) * num_epochs\n    print(f\"Total training steps: {total_steps}\")\n    # Adjust warmup_steps if desired (e.g., make it proportional to total_steps or one epoch)\n    # warmup_steps = len(train_loader) # Example: Warmup for 1 epoch\n    print(f\"Warmup steps: {warmup_steps}\")\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\nelse:\n    print(\"Warning: Train loader has zero length. Cannot create scheduler.\")\n    total_steps = 0\n    scheduler = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:17.103687Z","iopub.execute_input":"2025-05-07T09:54:17.103948Z","iopub.status.idle":"2025-05-07T09:54:17.116016Z","shell.execute_reply.started":"2025-05-07T09:54:17.103923Z","shell.execute_reply":"2025-05-07T09:54:17.115264Z"}},"outputs":[{"name":"stdout","text":"Train loader size: 93 batches\nTest loader size: 91 batches\nTotal training steps: 1395\nWarmup steps: 100\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def train_one_epoch(model, loader, loss_fn, optimizer, device, epoch, scheduler=None, grad_clip_norm=1.0): # grad_clip_norm is set\n    model.train()\n    if not loader:\n        print(f\"Skipping training for epoch {epoch} due to invalid loader.\")\n        return 0.0\n\n    pbar = tqdm(loader, desc=f\"Epoch {epoch} Training\")\n    total_loss = 0.0\n    steps = 0\n\n    for i, batch in enumerate(pbar):\n        if batch is None:\n            # print(f\"Skipping problematic batch {i}\") # Keep optional for debugging\n            continue\n        images, texts = batch\n        images = images.to(device, non_blocking=True)\n        texts = texts.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        try:\n            image_features, text_features, logit_scale = model(images, texts)\n        except Exception as e:\n            print(f\"Error during model forward pass in batch {i}: {e}\")\n            continue\n\n        if image_features is None or text_features is None:\n             print(f\"Skipping batch {i} due to None features\")\n             continue\n\n        # *** Optimization: Ensure loss call is correct ***\n        loss = loss_fn(image_features, text_features, logit_scale) # Use raw logit_scale\n\n        loss.backward()\n\n        # *** Optimization: Ensure Gradient Clipping is active ***\n        if grad_clip_norm is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n\n        optimizer.step()\n\n        # *** Optimization: Ensure Scheduler steps each batch ***\n        if scheduler:\n            scheduler.step() # Step scheduler per batch\n\n        total_loss += loss.item()\n        steps += 1\n        current_lr = optimizer.param_groups[0]['lr'] # Get current LR\n        pbar.set_postfix({\"Loss\": loss.item(), \"Avg Loss\": total_loss / steps, \"LR\": f\"{current_lr:.2e}\"}) # Display LR\n\n    avg_loss = total_loss / steps if steps > 0 else 0\n    print(f\"Epoch {epoch} Average Training Loss: {avg_loss:.4f}\")\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:17.116838Z","iopub.execute_input":"2025-05-07T09:54:17.117123Z","iopub.status.idle":"2025-05-07T09:54:17.157958Z","shell.execute_reply.started":"2025-05-07T09:54:17.117089Z","shell.execute_reply":"2025-05-07T09:54:17.157243Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Evaluation function remains the same as in the previous step\n# Ensure it uses preprocess_val for the test_dataset_eval\ndef evaluate_zeroshot(model, loader, tokenizer, all_class_names, device): # Changed arg to all_class_names\n    model.eval()\n    all_image_features = []\n    all_labels = []\n\n    # Encode all class prompts once\n    print(\"Encoding text prompts for evaluation...\")\n    with torch.no_grad():\n        # Use the first template for zero-shot evaluation consistency\n        prompts_for_eval = [create_prompt(c, prompt_templates[0]) for c in all_class_names]\n        text_inputs = tokenizer(prompts_for_eval).to(device)\n        class_text_features = model.encode_text(text_inputs)\n        class_text_features = F.normalize(class_text_features, dim=-1)\n    print(\"Text prompts encoded.\")\n\n    # Check if loader is valid\n    if not loader:\n        print(\"Evaluation loader is invalid. Skipping evaluation.\")\n        return 0.0\n\n    pbar = tqdm(loader, desc=\"Evaluating\")\n    with torch.no_grad():\n        for i, batch in enumerate(pbar):\n             # Handle potential errors from dataset loading/collation\n            if batch is None:\n                print(f\"Skipping problematic batch {i} in evaluation\")\n                continue\n            images, labels = batch\n            images = images.to(device, non_blocking=True)\n\n            try:\n                image_features = model.encode_image(images)\n                if image_features is None:\n                   print(f\"Skipping batch {i} due to None image features\")\n                   continue\n                image_features = F.normalize(image_features, dim=-1)\n\n                all_image_features.append(image_features.cpu())\n                all_labels.append(labels.cpu())\n            except Exception as e:\n                print(f\"Error during image encoding in batch {i}: {e}\")\n                continue\n\n    if not all_image_features:\n        print(\"No image features were collected for evaluation.\")\n        return 0.0\n\n    all_image_features = torch.cat(all_image_features)\n    all_labels = torch.cat(all_labels)\n\n    # Calculate similarities and predict\n    try:\n        logit_scale = model.logit_scale.exp().item() # Get current temperature\n    except AttributeError:\n         # Fallback if logit_scale is not learnable or directly accessible\n         print(\"Warning: Could not get learnable logit_scale. Using default 100.\")\n         logit_scale = 100.0 # Default CLIP value\n\n    similarities = (logit_scale * all_image_features.to(device) @ class_text_features.T).softmax(dim=-1)\n    _, predictions = similarities.topk(1, dim=-1)\n    predictions = predictions.squeeze().cpu()\n\n    accuracy = accuracy_score(all_labels.numpy(), predictions.numpy())\n    print(f\"Zero-shot Accuracy: {accuracy * 100:.2f}%\")\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:17.158615Z","iopub.execute_input":"2025-05-07T09:54:17.158781Z","iopub.status.idle":"2025-05-07T09:54:17.172731Z","shell.execute_reply.started":"2025-05-07T09:54:17.158768Z","shell.execute_reply":"2025-05-07T09:54:17.172055Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# *** Optimization: Use increased epochs ***\nnum_epochs = 15\nbest_accuracy = 0.0\noutput_dir = \"/kaggle/working/\"\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"--- Initial Zero-Shot Evaluation ---\")\ninitial_accuracy = evaluate_zeroshot(model, test_loader_eval, tokenizer, all_class_names, device)\nprint(\"-----------------------------------\\n\")\nbest_accuracy = initial_accuracy # Initialize best accuracy\n\nfor epoch in range(num_epochs):\n    epoch_num = epoch + 1\n    print(f\"\\n--- Starting Epoch {epoch_num}/{num_epochs} ---\")\n    # Pass scheduler and grad clip norm\n    # *** Optimization: Pass the created scheduler ***\n    train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device, epoch_num, scheduler, grad_clip_norm=1.0)\n\n    print(f\"--- Evaluating after Epoch {epoch_num} ---\")\n    current_accuracy = evaluate_zeroshot(model, test_loader_eval, tokenizer, all_class_names, device)\n\n    if current_accuracy > best_accuracy:\n        best_accuracy = current_accuracy\n        print(f\"*** New best accuracy: {best_accuracy*100:.2f}%. Saving model... ***\")\n        save_path = os.path.join(output_dir, f'flyp_clip_cub_best_epoch_{epoch_num}_acc{best_accuracy:.3f}.pt')\n        torch.save({\n            'epoch': epoch_num,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': train_loss,\n            'accuracy': best_accuracy,\n            'model_name': model_name,\n            'pretrained_dataset': pretrained_dataset\n        }, save_path)\n        print(f\"Model saved to {save_path}\")\n    else:\n         print(f\"Accuracy ({current_accuracy*100:.2f}%) did not improve from best ({best_accuracy*100:.2f}%). Not saving.\")\n\n    print(\"-----------------------------------\\n\")\n\nprint(f\"\\nFine-tuning finished. Best zero-shot accuracy achieved: {best_accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:54:17.173533Z","iopub.execute_input":"2025-05-07T09:54:17.173803Z","execution_failed":"2025-05-07T13:18:49.111Z"}},"outputs":[{"name":"stdout","text":"--- Initial Zero-Shot Evaluation ---\nEncoding text prompts for evaluation...\nText prompts encoded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d39049e3d164551a4506ea68a0f5e58"}},"metadata":{}},{"name":"stdout","text":"Zero-shot Accuracy: 65.17%\n-----------------------------------\n\n\n--- Starting Epoch 1/15 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1 Training:   0%|          | 0/93 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29133b47e3bb4abc89b18cbefeb90c60"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 Average Training Loss: 0.9715\n--- Evaluating after Epoch 1 ---\nEncoding text prompts for evaluation...\nText prompts encoded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"330bad8e6caa49d285e51c7cdeb5633f"}},"metadata":{}},{"name":"stdout","text":"Zero-shot Accuracy: 67.83%\n*** New best accuracy: 67.83%. Saving model... ***\nModel saved to /kaggle/working/flyp_clip_cub_best_epoch_1_acc0.678.pt\n-----------------------------------\n\n\n--- Starting Epoch 2/15 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2 Training:   0%|          | 0/93 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e39cd0f7e6114c86ada715f24e7ccdde"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 Average Training Loss: 0.7580\n--- Evaluating after Epoch 2 ---\nEncoding text prompts for evaluation...\nText prompts encoded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edefad32c334439d9d40d4790eba4d15"}},"metadata":{}},{"name":"stdout","text":"Zero-shot Accuracy: 71.61%\n*** New best accuracy: 71.61%. Saving model... ***\nModel saved to /kaggle/working/flyp_clip_cub_best_epoch_2_acc0.716.pt\n-----------------------------------\n\n\n--- Starting Epoch 3/15 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3 Training:   0%|          | 0/93 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e52dc34080f43188329a9202e56ef41"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 Average Training Loss: 0.6451\n--- Evaluating after Epoch 3 ---\nEncoding text prompts for evaluation...\nText prompts encoded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcccdf7f8d29438ba483b3ff500e657c"}},"metadata":{}},{"name":"stdout","text":"Zero-shot Accuracy: 73.16%\n*** New best accuracy: 73.16%. Saving model... ***\nModel saved to /kaggle/working/flyp_clip_cub_best_epoch_3_acc0.732.pt\n-----------------------------------\n\n\n--- Starting Epoch 4/15 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4 Training:   0%|          | 0/93 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f150ef50661e4a06adcf164b2134d497"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 Average Training Loss: 0.5746\n--- Evaluating after Epoch 4 ---\nEncoding text prompts for evaluation...\nText prompts encoded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596e5bd4437d43de95765031185177ea"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}