{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5140550,"sourceType":"datasetVersion","datasetId":2534241}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install open_clip_torch torch torchvision torchaudio pandas scikit-learn tqdm ftfy regex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:30:04.729081Z","iopub.execute_input":"2025-05-07T09:30:04.729299Z","iopub.status.idle":"2025-05-07T09:31:21.348234Z","shell.execute_reply.started":"2025-05-07T09:30:04.729282Z","shell.execute_reply":"2025-05-07T09:31:21.347220Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport open_clip\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms # We might use CLIP's specific transforms\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split # If train_test_split.txt is not used\nfrom sklearn.metrics import accuracy_score\nimport re\nimport torch.optim as optim\n# Use either transformers scheduler or implement manually. Transformers is easier.\n# !pip install -q transformers # Add this if transformers not installed in Cell 3\nfrom transformers import get_cosine_schedule_with_warmup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:21.349412Z","iopub.execute_input":"2025-05-07T09:31:21.349737Z","iopub.status.idle":"2025-05-07T09:31:34.711197Z","shell.execute_reply.started":"2025-05-07T09:31:21.349706Z","shell.execute_reply":"2025-05-07T09:31:34.710390Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/cub2002011/CUB_200_2011/\"\nIMAGE_DIR = os.path.join(DATA_DIR, \"images\")\n\n# Load image paths and IDs\nimages_df = pd.read_csv(os.path.join(DATA_DIR, 'images.txt'), sep=' ', names=['img_id', 'filepath'])\n\n# Load image class labels\nimage_class_labels_df = pd.read_csv(os.path.join(DATA_DIR, 'image_class_labels.txt'), sep=' ', names=['img_id', 'class_id'])\n\n# Load class names\nclasses_df = pd.read_csv(os.path.join(DATA_DIR, 'classes.txt'), sep=' ', names=['class_id', 'classname'])\n# Preprocess class names (e.g., \"001.Black_footed_Albatross\" -> \"black footed albatross\")\nclasses_df['classname'] = classes_df['classname'].apply(lambda x: x.split('.')[-1].replace('_', ' ').lower())\n\n# Load train/test split\ntrain_test_split_df = pd.read_csv(os.path.join(DATA_DIR, 'train_test_split.txt'), sep=' ', names=['img_id', 'is_training_img'])\n\n# Merge dataframes\ndata_df = images_df.merge(image_class_labels_df, on='img_id')\ndata_df = data_df.merge(classes_df, on='class_id')\ndata_df = data_df.merge(train_test_split_df, on='img_id')\n\n# Split into train and test sets\ntrain_df = data_df[data_df['is_training_img'] == 1].reset_index(drop=True)\ntest_df = data_df[data_df['is_training_img'] == 0].reset_index(drop=True)\n\nprint(f\"Total images: {len(data_df)}\")\nprint(f\"Training images: {len(train_df)}\")\nprint(f\"Testing images: {len(test_df)}\")\nprint(f\"Number of classes: {classes_df['class_id'].nunique()}\")\ndata_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:34.712766Z","iopub.execute_input":"2025-05-07T09:31:34.713189Z","iopub.status.idle":"2025-05-07T09:31:34.814438Z","shell.execute_reply.started":"2025-05-07T09:31:34.713170Z","shell.execute_reply":"2025-05-07T09:31:34.813837Z"}},"outputs":[{"name":"stdout","text":"Total images: 11788\nTraining images: 5994\nTesting images: 5794\nNumber of classes: 200\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   img_id                                           filepath  class_id  \\\n0       1  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n1       2  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n2       3  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n3       4  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n4       5  001.Black_footed_Albatross/Black_Footed_Albatr...         1   \n\n                classname  is_training_img  \n0  black footed albatross                0  \n1  black footed albatross                1  \n2  black footed albatross                0  \n3  black footed albatross                1  \n4  black footed albatross                1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img_id</th>\n      <th>filepath</th>\n      <th>class_id</th>\n      <th>classname</th>\n      <th>is_training_img</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n      <td>1</td>\n      <td>black footed albatross</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Simple templates used in many CLIP papers\nprompt_templates = [\n    'a photo of a {}.',\n    'a bad photo of a {}.',\n    'a photo of the {}.',\n    'a picture of a {}.',\n    'a bird called {}.',\n    'the bird {}.',\n    'a type of bird called {}.',\n    '{} bird.',\n]\n\ndef create_prompt(class_name, template):\n    return template.format(class_name)\n\n# Create prompts for all classes for zero-shot evaluation\nall_class_names = classes_df['classname'].tolist()\nnum_classes = len(all_class_names)\nzeroshot_prompts = [create_prompt(c, prompt_templates[0]) for c in all_class_names]\nprint(f\"Example prompt: {zeroshot_prompts[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:34.815084Z","iopub.execute_input":"2025-05-07T09:31:34.815269Z","iopub.status.idle":"2025-05-07T09:31:34.820781Z","shell.execute_reply.started":"2025-05-07T09:31:34.815255Z","shell.execute_reply":"2025-05-07T09:31:34.819995Z"}},"outputs":[{"name":"stdout","text":"Example prompt: a photo of a black footed albatross.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class CUBDataset(Dataset):\n    def __init__(self, df, image_dir, transform, tokenizer, templates, augment_templates=False):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.templates = templates\n        self.augment_templates = augment_templates # Flag for template augmentation\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row['filepath'])\n        class_name = row['classname']\n        class_id = row['class_id'] - 1 # 0-indexed class ID\n\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image_tensor = self.transform(image)\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}. Using placeholder.\")\n            # Return a dummy tensor or handle appropriately\n            # Try loading a different image or return None to skip in collate_fn\n            try: # Attempt to load the first image as a fallback\n                 placeholder_path = os.path.join(self.image_dir, self.df.iloc[0]['filepath'])\n                 image = Image.open(placeholder_path).convert('RGB')\n                 image_tensor = self.transform(image)\n            except: # If even that fails, return zeros\n                 image_tensor = torch.zeros((3, 224, 224)) # Adjust size if needed\n\n        # Create prompt for this specific image\n        # For FLYP-style augmentation, choose a random template during training\n        if self.augment_templates and self.transform == preprocess_train: # Only augment for training\n             template = np.random.choice(self.templates)\n        else:\n             template = self.templates[0] # Default template otherwise\n\n        text_prompt = create_prompt(class_name, template)\n        # Note: open_clip tokenizer returns a tensor batch; we need the first element [0]\n        tokenized_text = self.tokenizer(text_prompt)[0]\n\n        return image_tensor, tokenized_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:34.821416Z","iopub.execute_input":"2025-05-07T09:31:34.821633Z","iopub.status.idle":"2025-05-07T09:31:34.841526Z","shell.execute_reply.started":"2025-05-07T09:31:34.821617Z","shell.execute_reply":"2025-05-07T09:31:34.840875Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# --- Model Setup ---\nmodel_name = 'ViT-B-32'\npretrained_dataset = 'laion2b_s34b_b79k'\n\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n    model_name,\n    pretrained=pretrained_dataset,\n    device=device,\n    jit=False\n)\ntokenizer = open_clip.get_tokenizer(model_name)\n\nfor param in model.parameters():\n    param.requires_grad = True\n\n# --- Loss Function ---\nloss_fn = open_clip.ClipLoss(\n    local_loss=False,\n    gather_with_grad=False,\n    cache_labels=True,\n    rank=0,\n    world_size=1\n)\n\n# --- Optimizer ---\n# Try a potentially lower LR for fine-tuning\nlearning_rate = 5e-7 # Lowered LR\nweight_decay = 0.1\nbetas = (0.9, 0.98)\neps = 1e-6\n\nparams_to_optimize = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = optim.AdamW(\n    params_to_optimize,\n    lr=learning_rate,\n    betas=betas,\n    eps=eps,\n    weight_decay=weight_decay\n)\n\n# --- Scheduler ---\n# Need total steps: Define num_epochs and calculate *after* creating train_loader\nnum_epochs = 10 # Increased epochs\n# Placeholder for total_steps, will be calculated after train_loader is ready\n# total_steps = len(train_loader) * num_epochs\nwarmup_steps = 100 # Number of warmup steps (e.g., ~1 epoch or fixed number)\n# scheduler will be created *after* train_loader in the next cell block\nscheduler = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:34.842340Z","iopub.execute_input":"2025-05-07T09:31:34.842612Z","iopub.status.idle":"2025-05-07T09:31:39.333793Z","shell.execute_reply.started":"2025-05-07T09:31:34.842588Z","shell.execute_reply":"2025-05-07T09:31:39.333003Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4002057ecef4c479ff290b0e5d462fd"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"batch_size = 64 # Adjust based on GPU memory\nnum_workers = 2 # Adjust based on system capability\n\n# --- Training Dataset ---\n# Enable template augmentation for training\ntrain_dataset = CUBDataset(train_df, IMAGE_DIR, preprocess_train, tokenizer, prompt_templates, augment_templates=True)\n\n# Define a simple collate function to handle potential None items from dataset\ndef safe_collate(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    if not batch:\n        return None # Return None if the whole batch is problematic\n    try:\n        return torch.utils.data.dataloader.default_collate(batch)\n    except Exception as e:\n        print(f\"Error in collate_fn: {e}\")\n        return None # Return None on collation error\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True, collate_fn=safe_collate)\n\n# --- Test Dataset for Evaluation (Zero-shot style) ---\nclass CUBTestDataset(Dataset):\n    def __init__(self, df, image_dir, transform):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row['filepath'])\n        class_id = row['class_id'] - 1 # 0-indexed\n\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image_tensor = self.transform(image)\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}. Using placeholder.\")\n            # Fallback logic\n            try:\n                placeholder_path = os.path.join(self.image_dir, self.df.iloc[0]['filepath'])\n                image = Image.open(placeholder_path).convert('RGB')\n                image_tensor = self.transform(image)\n            except:\n                image_tensor = torch.zeros((3, 224, 224))\n\n        return image_tensor, torch.tensor(class_id, dtype=torch.long)\n\ntest_dataset_eval = CUBTestDataset(test_df, IMAGE_DIR, preprocess_val)\ntest_loader_eval = DataLoader(test_dataset_eval, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, collate_fn=safe_collate)\n\nprint(f\"Train loader size: {len(train_loader)} batches\")\nprint(f\"Test loader size: {len(test_loader_eval)} batches\")\n\n# --- Create Scheduler (Now that train_loader is defined) ---\nif len(train_loader) > 0:\n    total_steps = len(train_loader) * num_epochs\n    print(f\"Total training steps: {total_steps}\")\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\nelse:\n    print(\"Warning: Train loader has zero length. Cannot create scheduler.\")\n    total_steps = 0\n    scheduler = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:39.334480Z","iopub.execute_input":"2025-05-07T09:31:39.335065Z","iopub.status.idle":"2025-05-07T09:31:39.345876Z","shell.execute_reply.started":"2025-05-07T09:31:39.335047Z","shell.execute_reply":"2025-05-07T09:31:39.344803Z"}},"outputs":[{"name":"stdout","text":"Train loader size: 93 batches\nTest loader size: 91 batches\nTotal training steps: 930\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def train_one_epoch(model, loader, loss_fn, optimizer, device, epoch, scheduler=None, grad_clip_norm=1.0): # Added grad_clip_norm\n    model.train()\n    # Check if loader is valid\n    if not loader:\n        print(f\"Skipping training for epoch {epoch} due to invalid loader.\")\n        return 0.0\n\n    pbar = tqdm(loader, desc=f\"Epoch {epoch} Training\")\n    total_loss = 0.0\n    steps = 0\n\n    for i, batch in enumerate(pbar):\n        # Handle potential errors from dataset loading/collation\n        if batch is None:\n            print(f\"Skipping problematic batch {i}\")\n            continue\n        images, texts = batch\n        images = images.to(device, non_blocking=True)\n        texts = texts.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        try:\n            image_features, text_features, logit_scale = model(images, texts)\n        except Exception as e:\n            print(f\"Error during model forward pass in batch {i}: {e}\")\n            continue # Skip batch on forward error\n\n        # Ensure features are valid\n        if image_features is None or text_features is None:\n             print(f\"Skipping batch {i} due to None features\")\n             continue\n\n        # Calculate contrastive loss (pass raw logit_scale)\n        loss = loss_fn(image_features, text_features, logit_scale) # Use raw logit_scale\n\n        loss.backward()\n\n        # Gradient Clipping\n        if grad_clip_norm is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n\n        optimizer.step()\n\n        if scheduler:\n            scheduler.step() # Step scheduler\n\n        total_loss += loss.item()\n        steps += 1\n        current_lr = optimizer.param_groups[0]['lr'] # Get current LR\n        pbar.set_postfix({\"Loss\": loss.item(), \"Avg Loss\": total_loss / steps, \"LR\": f\"{current_lr:.2e}\"}) # Display LR\n\n    avg_loss = total_loss / steps if steps > 0 else 0\n    print(f\"Epoch {epoch} Average Training Loss: {avg_loss:.4f}\")\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:39.346805Z","iopub.execute_input":"2025-05-07T09:31:39.347031Z","iopub.status.idle":"2025-05-07T09:31:39.371215Z","shell.execute_reply.started":"2025-05-07T09:31:39.347000Z","shell.execute_reply":"2025-05-07T09:31:39.370597Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Evaluation function remains the same as in the previous step\n# Ensure it uses preprocess_val for the test_dataset_eval\ndef evaluate_zeroshot(model, loader, tokenizer, all_class_names, device): # Changed arg to all_class_names\n    model.eval()\n    all_image_features = []\n    all_labels = []\n\n    # Encode all class prompts once\n    print(\"Encoding text prompts for evaluation...\")\n    with torch.no_grad():\n        # Use the first template for zero-shot evaluation consistency\n        prompts_for_eval = [create_prompt(c, prompt_templates[0]) for c in all_class_names]\n        text_inputs = tokenizer(prompts_for_eval).to(device)\n        class_text_features = model.encode_text(text_inputs)\n        class_text_features = F.normalize(class_text_features, dim=-1)\n    print(\"Text prompts encoded.\")\n\n    # Check if loader is valid\n    if not loader:\n        print(\"Evaluation loader is invalid. Skipping evaluation.\")\n        return 0.0\n\n    pbar = tqdm(loader, desc=\"Evaluating\")\n    with torch.no_grad():\n        for i, batch in enumerate(pbar):\n             # Handle potential errors from dataset loading/collation\n            if batch is None:\n                print(f\"Skipping problematic batch {i} in evaluation\")\n                continue\n            images, labels = batch\n            images = images.to(device, non_blocking=True)\n\n            try:\n                image_features = model.encode_image(images)\n                if image_features is None:\n                   print(f\"Skipping batch {i} due to None image features\")\n                   continue\n                image_features = F.normalize(image_features, dim=-1)\n\n                all_image_features.append(image_features.cpu())\n                all_labels.append(labels.cpu())\n            except Exception as e:\n                print(f\"Error during image encoding in batch {i}: {e}\")\n                continue\n\n    if not all_image_features:\n        print(\"No image features were collected for evaluation.\")\n        return 0.0\n\n    all_image_features = torch.cat(all_image_features)\n    all_labels = torch.cat(all_labels)\n\n    # Calculate similarities and predict\n    try:\n        logit_scale = model.logit_scale.exp().item() # Get current temperature\n    except AttributeError:\n         # Fallback if logit_scale is not learnable or directly accessible\n         print(\"Warning: Could not get learnable logit_scale. Using default 100.\")\n         logit_scale = 100.0 # Default CLIP value\n\n    similarities = (logit_scale * all_image_features.to(device) @ class_text_features.T).softmax(dim=-1)\n    _, predictions = similarities.topk(1, dim=-1)\n    predictions = predictions.squeeze().cpu()\n\n    accuracy = accuracy_score(all_labels.numpy(), predictions.numpy())\n    print(f\"Zero-shot Accuracy: {accuracy * 100:.2f}%\")\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:39.373437Z","iopub.execute_input":"2025-05-07T09:31:39.373664Z","iopub.status.idle":"2025-05-07T09:31:39.393504Z","shell.execute_reply.started":"2025-05-07T09:31:39.373647Z","shell.execute_reply":"2025-05-07T09:31:39.392742Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_epochs = 15 # Increased epochs\nbest_accuracy = 0.0\noutput_dir = \"/kaggle/working/\"\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"--- Initial Zero-Shot Evaluation ---\")\n# Pass all_class_names instead of pre-generated prompts\ninitial_accuracy = evaluate_zeroshot(model, test_loader_eval, tokenizer, all_class_names, device)\nprint(\"-----------------------------------\\n\")\nbest_accuracy = initial_accuracy # Initialize best accuracy with the pre-fine-tuning score\n\nfor epoch in range(num_epochs):\n    epoch_num = epoch + 1\n    print(f\"\\n--- Starting Epoch {epoch_num}/{num_epochs} ---\")\n    # Pass scheduler and grad clip norm\n    train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device, epoch_num, scheduler, grad_clip_norm=1.0)\n\n    print(f\"--- Evaluating after Epoch {epoch_num} ---\")\n    # Pass all_class_names instead of pre-generated prompts\n    current_accuracy = evaluate_zeroshot(model, test_loader_eval, tokenizer, all_class_names, device)\n\n    # Only save if accuracy *improves* compared to the current best\n    if current_accuracy > best_accuracy:\n        best_accuracy = current_accuracy\n        print(f\"*** New best accuracy: {best_accuracy*100:.2f}%. Saving model... ***\")\n        save_path = os.path.join(output_dir, f'flyp_clip_cub_best_epoch_{epoch_num}_acc{best_accuracy:.3f}.pt')\n        # Save model state for potential resuming or inference\n        torch.save({\n            'epoch': epoch_num,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': train_loss, # Last epoch's train loss\n            'accuracy': best_accuracy, # Best accuracy so far\n            'model_name': model_name,\n            'pretrained_dataset': pretrained_dataset\n        }, save_path)\n        print(f\"Model saved to {save_path}\")\n    else:\n         print(f\"Accuracy ({current_accuracy*100:.2f}%) did not improve from best ({best_accuracy*100:.2f}%). Not saving.\")\n\n    print(\"-----------------------------------\\n\")\n\nprint(f\"\\nFine-tuning finished. Best zero-shot accuracy achieved: {best_accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T09:31:39.394418Z","iopub.execute_input":"2025-05-07T09:31:39.394695Z"}},"outputs":[{"name":"stdout","text":"--- Initial Zero-Shot Evaluation ---\nEncoding text prompts for evaluation...\nText prompts encoded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09d34edad2df473b9243ad58bce823d7"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}